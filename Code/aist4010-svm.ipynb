{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8083469,"sourceType":"datasetVersion","datasetId":4560625}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom glob import glob\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import RegexpTokenizer\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2024-04-17T16:42:51.075274Z","iopub.execute_input":"2024-04-17T16:42:51.075794Z","iopub.status.idle":"2024-04-17T16:42:51.084792Z","shell.execute_reply.started":"2024-04-17T16:42:51.075757Z","shell.execute_reply":"2024-04-17T16:42:51.082631Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Load the datasets\ntrain = pd.read_csv('/kaggle/input/m4-dataset/train_balanced.csv', dtype={'text': str, 'label': str})\nval = pd.read_csv('/kaggle/input/m4-dataset/val_balanced.csv', dtype={'text': str, 'label': str})\ntest = pd.read_csv('/kaggle/input/m4-dataset/test_balanced.csv', dtype={'text': str, 'label': str})\ntest_MGTBench = pd.read_csv('/kaggle/input/m4-dataset/test_MGTBench.csv', dtype={'text': str, 'label': str})\ntest_MixSet = pd.read_csv('/kaggle/input/m4-dataset/test_MixSet.csv', dtype={'text': str, 'label': str})\n\n\n# ************************* # \n# Please don't directly load all the training data for SVM, it will work extermely long for doing the training + testing, it is basically unreproducable \n# With that setting\n# Please just make the training set smaller before training (Still get similar result)\n# getting 10k / 15k data are both OK, depends on the time needed\n\n# train = train.head(15000)  # Using only the first 15000 entries\n# val = val.head(8000) # Used  if futher speedup the performance\n# ************************* # \n\n# Remove NaNs and reset index\ndatasets = [train, val, test, test_MGTBench, test_MixSet]\nfor dataset in datasets:\n    dataset.dropna(subset=['text'], inplace=True)\n    dataset.reset_index(drop=True, inplace=True)\n\n    \n# Combine the datasets for overall statistics\nall_data = pd.concat([train, val, test], ignore_index=True)\nall_data = test_MixSet\nprint(all_data.head())\nprint(all_data[all_data['label'] == 0].shape[0])\n# 1. Ratio of human and AI text\nplt.figure(figsize=(8, 6))\nsns.countplot(x='label', data=all_data)\nplt.title('Ratio of Human and AI Text')\nplt.xticks([0, 1], ['Human', 'AI'])\nplt.xlabel('Type of Text')\nplt.ylabel('Count')\nplt.show()\n\n# 2. Ratio of different domain sources\nplt.figure(figsize=(10, 8))\nsns.countplot(y='source', data=all_data, order = all_data['source'].value_counts().index)\nplt.title('Ratio of Different Domain Sources')\nplt.xlabel('Count')\nplt.ylabel('Source Domain')\nplt.show()\n\n# 3. Ratio of different models\nplt.figure(figsize=(10, 8))\nsns.countplot(y='model', data=all_data, order = all_data['model'].value_counts().index)\nplt.title('Ratio of Different Models')\nplt.xlabel('Count')\nplt.ylabel('Model')\nplt.show()\n\n# Print the number of entries for each model within each domain\nmodel_domain_counts = all_data.groupby(['source', 'model']).size().reset_index(name='count')\nprint(model_domain_counts)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SVM","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score, roc_curve, precision_score, recall_score\nfrom sklearn.svm import SVC\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom tqdm.auto import tqdm\nfrom collections import defaultdict\n\ndef evaluate_dataset(clf, X_texts, y_true, test_data, dataset_name, kernel):\n    print(f\"\\nEvaluating on {dataset_name} with {kernel} kernel...\")\n    X = vectorizer.transform(tqdm(X_texts, desc=f\"Vectorizing {dataset_name} Data\")).toarray()\n    y_pred = clf.predict(X)\n    y_proba = clf.predict_proba(X)[:, 1]  # Get probabilities for the positive class\n    \n    # Calculate metrics\n    accuracy = accuracy_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    precision = precision_score(y_true, y_pred, average='weighted')\n    recall = recall_score(y_true, y_pred, average='weighted')\n    auc = roc_auc_score(y_true, y_proba)\n    conf_matrix = confusion_matrix(y_true, y_pred)\n    \n    # Print overall results\n    print(f\"Results for {dataset_name}:\")\n    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, AUC Score: {auc:.4f}\")\n\n    # Store predictions and probabilities\n    results[(kernel, dataset_name)] = {\n        'y_true': y_true,\n        'y_pred': y_pred,\n        'y_proba': y_proba,\n        'Accuracy': accuracy,\n        'Precision': precision,\n        'Recall': recall,\n        'F1 Score': f1,\n        'AUC Score': auc,\n        'Confusion Matrix': conf_matrix.tolist()  # Convert numpy array to list for JSON serializable format\n    }\n\n\n    # Append domain-specific accuracy\n    domain_accuracies = {}\n    for domain in test_data['source'].unique():\n        domain_mask = (test_data['source'] == domain)\n\n        domain_acc = accuracy_score(y_true[domain_mask], y_pred[domain_mask])\n        domain_accuracies[domain] = domain_acc\n        print(f\"Accuracy for domain {domain} ({kernel} Kernel): {domain_acc:.4f}\")\n\n    # Append model-specific accuracy\n    model_accuracies = {}\n    for model_type in test_data['model'].unique():\n        model_mask = (test_data['model'] == model_type)\n        model_acc = accuracy_score(y_true[model_mask], y_pred[model_mask])\n        model_accuracies[model_type] = model_acc\n        print(f\"Accuracy for model {model_type} ({kernel} Kernel): {model_acc:.4f}\")\n\n    # Save predictions, probabilities, and metrics for each kernel and dataset\n    predictions_df = pd.DataFrame({'Actual': y_true, 'Predicted': y_pred, 'Probability': y_proba})\n    predictions_df.to_csv(f'{output_directory}/{kernel}_{dataset_name}_predictions.csv', index=False)\n    \n    metrics_df = pd.DataFrame({\n        'Metric': ['Accuracy', 'Precision', 'Recall', 'Precision', 'F1 Score', 'AUC Score'],\n        'Value': [accuracy, precision, recall, precision, f1, auc]\n    })\n    metrics_df.to_csv(f'{output_directory}/{kernel}_{dataset_name}_metrics.csv', index=False)\n\n    # Save domain and model accuracies\n    domain_df = pd.DataFrame(list(domain_accuracies.items()), columns=['Domain', 'Accuracy'])\n    domain_df.to_csv(f'{output_directory}/{kernel}_{dataset_name}_domain_accuracies.csv', index=False)\n\n    model_df = pd.DataFrame(list(model_accuracies.items()), columns=['Model', 'Accuracy'])\n    model_df.to_csv(f'{output_directory}/{kernel}_{dataset_name}_model_accuracies.csv', index=False)\n\ndef evaluate_dataset_accuracies(clf, X_texts, y_true, test_data, dataset_name, kernel):\n    print(f\"\\nEvaluating on {dataset_name} with {kernel} kernel...\")\n    X = vectorizer.transform(tqdm(X_texts, desc=f\"Vectorizing {dataset_name} Data\")).toarray()\n    y_pred = clf.predict(X)\n    y_proba = clf.predict_proba(X)[:, 1]  # Get probabilities for the positive class\n    \n    # Initialize DataFrame from test_data for easier manipulation\n    test_data_df = pd.DataFrame(test_data)\n    test_data_df['Predicted'] = y_pred\n\n    # Dataset-specific accuracies\n    dataset_accuracies = {}\n    dataset_labels = {\n        'MixsetHUMANIZE': 'Humanize',\n        'MixsetCOMPLETE': 'Complete',\n        'MixsetREWRITE': 'Rewrite',\n        'MixsetPOLISH_SENTENCE': 'Polish'\n    }\n\n    for dataset, label in dataset_labels.items():\n        dataset_mask = (test_data['dataset'] == dataset)\n        dataset_acc = accuracy_score(y_true[dataset_mask], y_pred[dataset_mask])\n        dataset_accuracies[label] = dataset_acc\n        print(f\"Accuracy for method {label} ({kernel} Kernel): {dataset_acc:.4f}\")\n\n    return dataset_accuracies \n\n# Usage of evaluate_dataset function within the loop over kernels\nX_train_texts = train['text']\ny_train = train['label'].astype(int).values\nX_test_texts = test['text']\ny_test = test['label'].astype(int).values\nX_test_MGTBench_texts = test_MGTBench['text']\ny_test_MGTBench = test_MGTBench['label'].astype(int).values\nX_test_MixSet_texts = test_MixSet['text']\ny_test_MixSet = test_MixSet['label'].astype(int).values\n\nvectorizer = TfidfVectorizer(max_features=650)\nX_train = vectorizer.fit_transform(tqdm(X_train_texts, desc=\"Vectorizing Train Data\")).toarray()\n\n# Initialize your metrics and results storage\nkernels = ['rbf', 'linear', 'poly', 'sigmoid']\nresults = {}\noutput_directory = '/kaggle/working'  # Change this to your desired path\n\nfor kernel in kernels:\n    clf = SVC(kernel=kernel, probability=True)\n    clf.fit(X_train, y_train)\n\n    # ************************* # \n    # The above ones are for general testing\n    # ************************* # \n    \n    # Evaluate on standard test set\n    evaluate_dataset(clf, test['text'], test['label'].astype(int).values, test, \"standard_test\", kernel)\n    \n    # Evaluate on MGTBench\n    evaluate_dataset(clf, test_MGTBench['text'], test_MGTBench['label'].astype(int).values, test_MGTBench, \"MGTBench\", kernel)\n    \n    # Evaluate on MixSet\n    evaluate_dataset(clf, test_MixSet['text'], test_MixSet['label'].astype(int).values, test_MixSet, \"MixSet\", kernel)\n    \n    \n    # ************************* # \n    # While this one is for testing different methods in MixSet\n    # ************************* # \n    \n    test_MixSet_GPT4 = test_MixSet[test_MixSet['model'] == 'GPT4']\n    #print(test_MixSet_GPT4)\n    evaluate_dataset_accuracies(clf, test_MixSet_GPT4['text'], test_MixSet_GPT4['label'].astype(int).values, test_MixSet_GPT4, \"MixSet_GPT4\", kernel)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T09:34:26.268777Z","iopub.execute_input":"2024-04-16T09:34:26.269178Z","iopub.status.idle":"2024-04-16T10:27:05.663993Z","shell.execute_reply.started":"2024-04-16T09:34:26.269148Z","shell.execute_reply":"2024-04-16T10:27:05.662641Z"},"trusted":true},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"Vectorizing Train Data:   0%|          | 0/14706 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7d61127c697469e90a4505b4b8e1ccf"}},"metadata":{}},{"name":"stdout","text":"\nEvaluating on MixSet_GPT4 with rbf kernel...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Vectorizing MixSet_GPT4 Data:   0%|          | 0/765 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60df466290384bffb42a8464fafb0d55"}},"metadata":{}},{"name":"stdout","text":"Accuracy for method Humanize (rbf Kernel): 0.6433\nAccuracy for method Complete (rbf Kernel): 0.9806\nAccuracy for method Rewrite (rbf Kernel): 0.9935\nAccuracy for method Polish (rbf Kernel): 0.9806\n\nEvaluating on MixSet_GPT4 with linear kernel...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Vectorizing MixSet_GPT4 Data:   0%|          | 0/765 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd13430ae42641b69eea361e80333453"}},"metadata":{}},{"name":"stdout","text":"Accuracy for method Humanize (linear Kernel): 0.6367\nAccuracy for method Complete (linear Kernel): 0.9806\nAccuracy for method Rewrite (linear Kernel): 0.9613\nAccuracy for method Polish (linear Kernel): 0.9419\n\nEvaluating on MixSet_GPT4 with poly kernel...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Vectorizing MixSet_GPT4 Data:   0%|          | 0/765 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e1a76272efb461a835af04e0e8a362e"}},"metadata":{}},{"name":"stdout","text":"Accuracy for method Humanize (poly Kernel): 0.5567\nAccuracy for method Complete (poly Kernel): 0.9677\nAccuracy for method Rewrite (poly Kernel): 0.9935\nAccuracy for method Polish (poly Kernel): 0.9677\n\nEvaluating on MixSet_GPT4 with sigmoid kernel...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Vectorizing MixSet_GPT4 Data:   0%|          | 0/765 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1664041c4cbb4175aaa8d05bae1fc84e"}},"metadata":{}},{"name":"stdout","text":"Accuracy for method Humanize (sigmoid Kernel): 0.5867\nAccuracy for method Complete (sigmoid Kernel): 0.9742\nAccuracy for method Rewrite (sigmoid Kernel): 0.9419\nAccuracy for method Polish (sigmoid Kernel): 0.9097\n","output_type":"stream"}]}]}